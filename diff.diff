diff --git a/countTokens.py b/countTokens.py
index 5b8c772..0794dae 100644
--- a/countTokens.py
+++ b/countTokens.py
@@ -1,48 +1,79 @@
+import os
 import sys
+import argparse
 try:
-    import tiktoken # type: ignore
+    import tiktoken  # type: ignore
 except ImportError:
     print("Error: Failed to import 'tiktoken'. Please make sure it is installed.")
     sys.exit(1)
 
+# Hardcoded token limits for each model
+MODEL_LIMITS = {
+    'gpt-4': 8192,
+    'gpt-4o': 8192,
+    'gpt-4o-mini': 8192,
+}
 
-def count_tokens_from_file(file_path, encoding):
-    with open(file_path, 'r', encoding='utf-8') as f:
-        content = f.read()
-    tokens = encoding.encode(content)
-    return len(tokens)
+def gather_files(directory_path):
+    """Gather all files recursively from the given directory."""
+    all_files = []
+    for root, _, filenames in os.walk(directory_path):
+        for filename in filenames:
+            all_files.append(os.path.join(root, filename))
+    return all_files
 
+def count_tokens_from_file(file_path, encoding):
+    """Count the number of tokens in the provided file using specified encoding."""
+    try:
+        with open(file_path, 'r', encoding='utf-8') as f:
+            content = f.read()
+        tokens = encoding.encode(content)
+        return len(tokens)
+    except Exception as e:
+        print(f"Error processing file {file_path}: {e}")
+        return None
 
 def main():
-    # Check if file path is provided
-    if len(sys.argv) < 2:
-        print("Usage: python countTokens.py <input_file> [model]")
-        print("Model options: gpt-3.5, gpt-4, gpt-4o, gpt-4o-mini")
-        print("Default model is 'gpt-4'.")
-        return
-
-    file_path = sys.argv[1]
-    model = sys.argv[2] if len(sys.argv) > 2 else 'gpt-4'
-
-    # Set encoding and context length based on model
-    if model == 'gpt-3.5':
-        encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')
-        context_length = 16000
-    elif model in ['gpt-4', 'gpt-4o', 'gpt-4o-mini']:
-        encoding = tiktoken.encoding_for_model('gpt-4')
-        context_length = 128000
-    else:
-        print("Unsupported model.")
-        return
+    parser = argparse.ArgumentParser(description='Token Counter for Files or Directories')
+    parser.add_argument('path', type=str, help='Path to file or directory to count tokens')
+    parser.add_argument('--model', default='gpt-4', choices=MODEL_LIMITS.keys(), help='Model to use for token counting')
 
-    # Get the token count
-    token_count = count_tokens_from_file(file_path, encoding)
+    args = parser.parse_args()
 
-    # Check if the token count fits within the context window
-    if token_count > context_length:
-        print(f"TOO BIG: '{file_path}' contains {token_count} tokens, exceeding {model}'s {context_length} token context window.")
+    # Set encoding based on model
+    encoding = tiktoken.encoding_for_model(args.model)
+
+    if os.path.isdir(args.path):
+        # If it's a directory, gather all files and count tokens for each
+        files = gather_files(args.path)
     else:
-        print(f"SUCCESS: '{file_path}' contains {token_count} tokens, fitting within {model}'s {context_length} token context window.")
+        # If it's a single file, just use that file
+        files = [args.path]
+
+
+    # Print header
+    print("TOKENS | FILENAME")
+
+    total_tokens = 0
+    # Iterate through files and count tokens
+    for file_path in files:
+        token_count = count_tokens_from_file(file_path, encoding)
+        if token_count is not None:
+            total_tokens += token_count
+            space_padding = 6 - len(str(token_count))
+            print(f"{' ' * space_padding}{token_count} | {file_path}")
+
+
+    print("_______|____________________")
+
+    # Print total token count
+    space_padding = 6 - len(str(total_tokens))
+    print(f"{' ' * space_padding}{total_tokens} | Total Tokens")
+
+    # Print the model's max input size
+    max_length = MODEL_LIMITS[args.model]
+    space_padding = 6 - len(str(max_length))
+    print(f"{' ' * space_padding}{max_length} | {args.model} max input size")
 
 if __name__ == "__main__":
     main()
diff --git a/makePrompt.py b/makePrompt.py
index 0e19eaf..505f77f 100644
--- a/makePrompt.py
+++ b/makePrompt.py
@@ -18,23 +18,24 @@ def count_tokens(content, encoding):
     except Exception:
         return 0
 
-def prefix_with_line_numbers(content):
-    """Prefix each line of content with its line number."""
-    return "\n".join(f"{i + 1}|{line}" for i, line in enumerate(content.splitlines()))
+def prefix_with_line_numbers(content, line_numbers):
+    """Prefix each line of content with its line number if requested."""
+    if line_numbers:
+        return "\n".join(f"{i + 1}|{line}" for i, line in enumerate(content.splitlines()))
+    return content
 
-# Content Processing Functions
-def remove_non_crucial_lines(content, extension):
-    """Remove non-crucial lines like imports, package declarations."""
-    lines = content.splitlines()
+def remove_non_crucial_lines(content, extension, concise):
+    """Remove non-crucial lines like imports, package declarations if requested."""
+    if not concise:
+        return content
 
-    # Determine if every line has a prefix (and hence is a prefixed content)
+    lines = content.splitlines()
     prefixed_content = all('|' in line for line in lines)
 
     def get_content_of_line(line):
         """Retrieve the actual content of the line, excluding any line number prefix."""
         return line.split('|', 1)[1].strip() if prefixed_content else line.strip()
 
-    # Removing package declarations and imports for common languages
     import_startswith = {
         '.php': 'use ',
         '.js': 'import ',
@@ -46,37 +47,67 @@ def remove_non_crucial_lines(content, extension):
 
     return '\n'.join(lines)
 
-def remove_comments(content, single, multi_start=None, multi_end=None):
-    """Remove comments from content."""
-    if multi_start and multi_end:
-        content = re.sub(re.escape(multi_start) + r'.*?' + re.escape(multi_end), '', content, flags=re.DOTALL)
-    content = re.sub(re.escape(single) + r'.*$', '', content, flags=re.MULTILINE)
+def detect_full_extension(filename):
+    """Detect the full extension"""
+    # Handle .env files explicitly
+    if os.path.basename(filename) == '.env':
+        return '.env'
+
+    # Handle compound extensions like .blade.php.
+    _, ext = os.path.splitext(filename)
+    if ext == '.php' and filename.endswith('.blade.php'):
+        return '.blade.php'
+    return ext
+
+def remove_comments(content, extension, keep_comments):
+    """Remove comments from content if not keeping them."""
+    if keep_comments:
+        return content
+
+    # Detect full file extension, including compound extensions like .blade.php
+    full_extension = detect_full_extension(extension)
+
+    # Debugging: Output the detected extension
+    if args.debug:
+        print(f"Debug: Detected file extension as '{full_extension}' for file '{extension}'")
+
+    comment_rules = {
+        '.php': ('//', '/*', '*/'),
+        '.js': ('//', '/*', '*/'),
+        '.css': ('/*', '*/'),
+        '.blade.php': ('{{--', '--}}', '<!--', '-->'),
+        '.env': ('#', None, None),
+    }
+
+    if full_extension in comment_rules:
+        if full_extension == '.blade.php':
+            # Remove Blade-style comments {{-- ... --}}
+            content = re.sub(r'\{\{--.*?--\}\}', '', content, flags=re.DOTALL)
+            # Remove HTML-style comments <!-- ... -->
+            content = re.sub(r'<!--.*?-->', '', content, flags=re.DOTALL)
+        else:
+            single, multi_start, multi_end = comment_rules[full_extension]
+            # Remove multiline comments
+            if (multi_start and multi_end):
+                content = re.sub(re.escape(multi_start) + r'.*?' + re.escape(multi_end), '', content, flags=re.DOTALL)
+            # Remove single-line comments
+            content = re.sub(re.escape(single) + r'.*$', '', content, flags=re.MULTILINE)
+
     return content
 
 def clean_content(content):
+    """Clean content by removing trailing spaces and empty lines."""
     lines = content.splitlines()
-    prefixed_content = all('|' in line for line in lines)
-
-    trimmed_lines = []
-    for line in lines:
-        trimmed_line = line.rstrip()
-        if prefixed_content:
-            if trimmed_line.split('|', 1)[1].strip():
-                trimmed_lines.append(trimmed_line)
-        else:
-            if trimmed_line.strip():
-                trimmed_lines.append(trimmed_line)
-
+    trimmed_lines = [line.rstrip() for line in lines if line.strip()]
     return "\n".join(trimmed_lines)
 
-# Concatenation Function
-def concat_files(filenames, output_base, max_tokens=None, keep_comments=False):
+def concat_files(filenames, output_base, max_tokens, keep_comments, line_numbers, show_full_path, encoding):
     """Concatenate content from given filenames and handle comments."""
     concatenated_content = ""
     current_part = 1
     current_tokens = 0
 
-    print("Tokens | File")
+    print("TOKENS | FILENAME")
 
     for filename in filenames:
         code_file = filename.endswith(tuple(CODE_FILES))
@@ -84,44 +115,35 @@ def concat_files(filenames, output_base, max_tokens=None, keep_comments=False):
             with open(filename, 'r', encoding='utf-8') as file:
                 content = file.read()
                 if not content.strip():
-                    print(f"Skipped empty file: {filename}")
+                    print(f"       | {filename}")
                     continue
 
-                if code_file and args.line_numbers:
-                    content = prefix_with_line_numbers(content)
-
-                if not keep_comments:
-                    comment_rules = {
-                        '.php': ('//', '/*', '*/'),
-                        '.js': ('//', '/*', '*/'),
-                        '.css': ('/*', '*/'),
-                    }
-                    for exts, rules in comment_rules.items():
-                        if filename.endswith(exts):
-                            content = remove_comments(content, *rules)
-                            break
+                # Full extension for proper processing
+                extension = filename
 
-                if args.concise:
-                    content = remove_non_crucial_lines(content, os.path.splitext(filename)[1])
+                # Debugging: Output the filename being processed
+                if args.debug:
+                    print(f"Debug: Processing file '{filename}'")
 
+                content = prefix_with_line_numbers(content, line_numbers)
+                content = remove_comments(content, extension, keep_comments)
+                content = remove_non_crucial_lines(content, extension, args.concise)
                 content = clean_content(content)
 
-                # Count tokens for the current file
                 file_tokens = count_tokens(content, encoding)
-                # Calculate the number of spaces needed to align the output
                 space_padding = 6 - len(str(file_tokens))
-                print(f"{file_tokens}{' ' * space_padding} | {filename}")  # Print token count for each file
+                print(f"{' ' * space_padding}{file_tokens} | {filename}")
 
-                current_tokens += count_tokens(content, encoding)
+                current_tokens += file_tokens
                 separator = "\n\n\n" if concatenated_content else ""
-                displayed_filename = filename if args.show_full_path else os.path.basename(filename)
+                displayed_filename = filename if show_full_path else os.path.basename(filename)
                 concatenated_content += f"{separator}{displayed_filename}:\n"
                 concatenated_content += "```\n" if code_file else "\"\n"
                 concatenated_content += f"{content}\n"
                 concatenated_content += "```\n" if code_file else "\"\n"
 
                 if max_tokens and current_tokens > max_tokens:
-                    output_path = f"{output_base}_part{current_part}"
+                    output_path = f"{output_base}_part{current_part}.txt"
                     with open(output_path, 'w', encoding='utf-8') as output_file:
                         output_file.write(concatenated_content)
 
@@ -133,38 +155,47 @@ def concat_files(filenames, output_base, max_tokens=None, keep_comments=False):
         except Exception as e:
             print(f"Error processing file {filename}: {e}")
 
-    output_filename = f"{output_base}" if current_part == 1 else f"{output_base}_part{current_part}"
-    with open(output_filename, 'w', encoding='utf-8') as output_file:
-        output_file.write(concatenated_content)
+    if concatenated_content:
+        output_filename = f"{output_base}_part{current_part}.txt" if current_part > 1 else f"{output_base}.txt"
+        with open(output_filename, 'w', encoding='utf-8') as output_file:
+            output_file.write(concatenated_content)
 
-    print("--- Processing complete ---")
-    print(f"Output file `{output_filename}` contains {current_tokens:,} tokens.")
-    return output_filename  # Return the final path for printing purposes.
+        # Print total token count
+        print("_______|____________________")
+        space_padding = 6 - len(str(current_tokens))
+        print(f"{' ' * space_padding}{current_tokens} | Output file `{output_filename}`")
+        return output_filename
 
-# Main Script Execution
 if __name__ == "__main__":
-    parser = argparse.ArgumentParser(description='Process and concatenate files.')
-    parser.add_argument('input_file', type=str, help='File containing list of paths or directory to search for files.')
-    parser.add_argument('-o', '--output', required=False, type=str, default="prompt.txt", help='Output file prefix. If not provided, it will overwrite the input file.')
+    parser = argparse.ArgumentParser(description='Process and concatenate files from given paths or directories.')
+    parser.add_argument('input_path', type=str, help='Directory path or file containing list of paths to search for files.')
+    parser.add_argument('-o', '--output', required=False, type=str, default="prompt", help='Output file prefix. Defaults to "prompt.txt" if not specified.')
     parser.add_argument('--line-numbers', action='store_true', help='Prefix lines with their line numbers.')
     parser.add_argument('--keep-comments', action='store_true', help='Retain comments in files.')
     parser.add_argument('--show-full-path', action='store_true', help='Display the full path of files.')
-    parser.add_argument('--max-tokens', required=False, type=int, help='Max tokens per output file.')
+    parser.add_argument('--max-tokens', required=False, type=int, help='Max tokens per output file part.')
     parser.add_argument('--concise', action='store_true', help='Remove non-crucial lines like imports and package names.')
+    parser.add_argument('--debug', action='store_true')
 
     args = parser.parse_args()
 
-    source_files = []
-    with open(args.input_file, 'r', encoding='utf-8') as f:
-        paths = [line.strip() for line in f if line.strip() and not line.startswith("#")]
-
-    for path in paths:
-        source_files.extend(gather_files(path) if os.path.isdir(path) else [path])
+    # Determine if the input is a directory or a file list
+    if os.path.isdir(args.input_path):
+        source_files = gather_files(args.input_path)
+    elif os.path.isfile(args.input_path):
+        with open(args.input_path, 'r', encoding='utf-8') as f:
+            paths = [line.strip() for line in f if line.strip() and not line.startswith("#")]
+        source_files = []
+        for path in paths:
+            source_files.extend(gather_files(path) if os.path.isdir(path) else [path])
+    else:
+        print("The specified input path does not exist.")
+        sys.exit(1)
 
     try:
         encoding = tiktoken.encoding_for_model('gpt-4')
     except Exception:
-        encoding = None
+        encoding = None  # Consider providing a fallback or default handling if tiktoken fails
 
-    output_base = args.output if args.output else os.path.splitext(args.input_file)[0]  # Use input_file's base if no output is provided
-    final_output = concat_files(source_files, output_base, args.max_tokens, args.keep_comments)
+    output_base = args.output if args.output else os.path.splitext(args.input_path)[0]  # Use input_path's base if no output is provided
+    final_output = concat_files(source_files, output_base, args.max_tokens, args.keep_comments, args.line_numbers, args.show_full_path, encoding)
